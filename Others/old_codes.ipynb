{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes cuando se trabajaba por chunks, o lista de dataframes. Ahora se utiliza la librería Dask en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el dataset de vacunados en una lista de chunks o dfs\n",
    "# (Para procesar información en bloques de 500 000) \n",
    "lst_vac = fn.df_into_chunks(df_vac)\n",
    "del df_vac\n",
    "\n",
    "[chunk.dropna(inplace=True) for chunk in lst_vac] # Drop na en caso de que existan\n",
    "\n",
    "# Que cada 'chunk' de lst_vac sea solamente cuando dosis = 2, es decir tengan 2 dosis\n",
    "lst_vac = [chunk.loc[chunk['dosis'] == '2'] for chunk in lst_vac]\n",
    "\n",
    "# Convertir a formato fecha la columna 'fecha vacunacion' y obtener su año y semana epi\n",
    "[fn.variable_fecha(chunk, 'fecha_vacunacion') for chunk in lst_vac]\n",
    "[fn.date_to_epiweek(chunk, 'fecha_vacunacion') for chunk in lst_vac]\n",
    "\n",
    "# Creamos columnas de 1 en cada chunk para contabilizar cada caso de vacunado\n",
    "for chunk in lst_vac:                                        \n",
    "    chunk['vacunado'] = 1\n",
    "    chunk['vacunado'].apply(np.int8)\n",
    "    del chunk['dosis']  # Borramos columna dosis llena de número 2\n",
    "del chunk # Borramos el último chunk que queda al final\n",
    "\n",
    "print(lst_vac[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epiweeks_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el total de VACUNADOS por semana y año \n",
    "    epidemiológico (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index=[chunk['epi_year'],\n",
    "                                                            chunk['epi_week']],\n",
    "                                                     columns=chunk['vacunado'])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "        \n",
    "    # Unimos todos los dfs sumados en uno solo\n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1) \n",
    "    epi_vac = pd.DataFrame(merged_epivac.sum(numeric_only=True, axis=1))\n",
    "    epi_vac.columns = ['vacunados']\n",
    "    \n",
    "    return epi_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vac_dose(vac_url):\n",
    "    \"\"\"\n",
    "    Función que toma la dirección del dataset de vacunados y devuelve el TOTAL DE DOSIS APLICADAS \n",
    "    (1ra,2nda y 3era dosis) por semana epidemiológica\n",
    "    \"\"\"\n",
    "    vac_cols = ['fecha_vacunacion', 'dosis']                     \n",
    "    df_vac = fn.read_largeCSV_file(vac_url, ',', vac_cols)    \n",
    "    lst_vac = fn.df_into_chunks(df_vac)               \n",
    " \n",
    "    for idx, chunk in enumerate(lst_vac):\n",
    "        chunk = fn.variable_fecha(chunk, 'fecha_vacunacion')\n",
    "\n",
    "    for idx, chunk in enumerate(lst_vac):\n",
    "        chunk = fn.date_to_epiweek(chunk, \"fecha_vacunacion\")\n",
    "        \n",
    "    return lst_vac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacDose_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el TOTAL DE VACUNADOS por DEPARTAMENTO \n",
    "    del PERÚ (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index = [chunk['epi_year'], chunk['epi_week']],\n",
    "                                                     columns = [chunk['dosis']])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "\n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1)  # Merge all dfs\n",
    "    merged_epivac = merged_epivac.fillna(0).astype(np.int64)\n",
    "    merged_epivac = merged_epivac.groupby(level=0, axis=1).sum()\n",
    "\n",
    "    return merged_epivac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes ya no usados para dataset de vacunados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Procesamos el dataset de vacunados\n",
    "\n",
    "### 3.1. Leemos el dataset\n",
    "\n",
    "#Debido al tamaño del dataset y para preservar recursos, no se procesará la información usando **Pandas**. En su lugar se utilizará el paquete **Dask**.\n",
    "\n",
    "vac_col = ['fecha_vacunacion','dosis']\n",
    "df_vac = dd.read_csv(vac_url, sep = \",\", usecols = vac_col, \n",
    "                     dtype = {'fecha_vacunacion':'int32', 'dosis': 'int8'})\n",
    "del vac_col\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos como vacunados solamente a personas con las 2 dosis\n",
    "df_vac = df_vac.loc[df_vac['dosis'] == 2]\n",
    "\n",
    "# Transformamos a formato fecha\n",
    "df_vac = df_vac.assign(fecha_vacunacion = dd.to_datetime(df_vac[\"fecha_vacunacion\"], format = \"%Y%m%d\", \n",
    "                                                         errors=\"coerce\"))\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el año y semana epidemiológica en una sola columna\n",
    "df_vac['epi_date'] = df_vac['fecha_vacunacion'].map(lambda date : Week.fromdate(date).isoformat())\n",
    "del df_vac['fecha_vacunacion'], df_vac['dosis']\n",
    "\n",
    "# Se separa la columna obtenida en 2, una para el año y otra para la semana epidemiológica\n",
    "df_vac[['year','epi_week']] = df_vac['epi_date'].str.split(\"W\", 1, expand=True)\n",
    "del df_vac['epi_date']\n",
    "df_vac['vacunados'] = 1\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vac['year'] = df_vac['year'].astype('int16')\n",
    "df_vac['epi_week'] = df_vac['epi_week'].astype('int8')\n",
    "df_vac['vacunados'] = df_vac['vacunados'].astype('int8')\n",
    "df_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosstab4dask(ddf):\n",
    "\n",
    "    lst = [] # Lista para almacenar la sumatoria de cada particion\n",
    "\n",
    "    for partition in range(0, ddf.npartitions):\n",
    "        lst.append(ddf.partitions[partition].compute().groupby(['year', 'epi_week']).count())\n",
    "\n",
    "    total_epivac = pd.concat(lst, axis=1).sum(numeric_only=True, axis=1).apply(np.int64)\n",
    "    del lst\n",
    "    total_epivac = pd.DataFrame(total_epivac)\n",
    "    total_epivac.columns = ['vacunados']\n",
    "    \n",
    "    return total_epivac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_vac = crosstab4dask(df_vac)\n",
    "del df_vac\n",
    "epi_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_epiweeks = pd.concat([epi_fal, epi_vac], axis=1)         \n",
    "merged_epiweeks = merged_epiweeks.fillna(value = 0) # Rellenamos vacíos o Nan values con 0\n",
    "del epi_fal, epi_vac\n",
    "\n",
    "# Cambiamos a int ya que existen Nan values y cambia a float64 automáticamente\n",
    "merged_epiweeks = merged_epiweeks.astype('int64')\n",
    "                                    \n",
    "merged_epiweeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_data_dep(falxdep_df):\n",
    "    \"\"\"Function to fix the indexes of the data of deceased by department of Peru.\n",
    "    IMPORTANT: There are more efficient ways to modify indexes using 'loc' and \n",
    "    'iloc' but this method at least 'works'\"\"\"\n",
    "\n",
    "    time = falxdep_df[[\"fallecido\", \"Unnamed: 1\"]]   # Get the col of epidemiological weeks\n",
    "    time = time.rename(columns=time.iloc[1])     # Put the first row (epi_week) as header\n",
    "    time = time.drop([0,1, len(time)-1],axis=0)  # Drop the first and last row (header, nan and total)\n",
    "    time = time.reset_index(drop=True)           # Reset index\n",
    "\n",
    "    departments = falxdep_df.drop([\"Unnamed: 1\", 'fallecido'], axis=1)    # Drop cols that are not departments\n",
    "    departments = departments.rename(columns=departments.iloc[0])     # Put the first row (epi_week) as header\n",
    "    departments = departments.drop([0,1, len(departments)-1],axis=0)  # Drop the first and last row (header, nan and total)\n",
    "    departments = departments.reset_index(drop=True)                  # Reset index\n",
    "\n",
    "    falxdep_df = pd.concat([time, departments], axis=1)\n",
    "    return falxdep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_falxdep_fix = filtering_data_dep(ct_falxdep)\n",
    "print(ct_falxdep_fix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vac_department(vac_url):\n",
    "    \"\"\"\n",
    "    Función que toma la dirección del dataset de vacunados y devuelve el número de VACUNADOS \n",
    "    por los 24 departamentos del Perú\n",
    "    \"\"\"\n",
    "    vac_col = ['id_centro_vacunacion', 'dosis']                     \n",
    "    df_vac = fn.read_largeCSV_file(vac_url, ',', vac_col)    \n",
    "    lst_vac = fn.df_into_chunks(df_vac)               \n",
    "    \n",
    "    # Que cada 'chunk' de lst_vac sea solamente cuando dosis = 2, es decir tengan 2 dosis\n",
    "    lst_vac = [chunk.loc[chunk.loc[:, 'dosis'] == 2] for chunk in lst_vac]\n",
    "\n",
    "    # Creamos columnas de 1 en cada chunk para contabilizar cada caso de vacunado\n",
    "    for chunk in lst_vac:                                        \n",
    "        chunk['vacunado'] = 1\n",
    "        chunk['vacunado'].apply(np.int8)\n",
    "        del chunk['dosis']  # Borramos columna dosis llena de número 2\n",
    "    del chunk # Borramos el último chunk que queda al final\n",
    "    \n",
    "    return lst_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacxdep_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el TOTAL DE VACUNADOS por DEPARTAMENTO \n",
    "    del PERÚ (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index=[chunk['id_centro_vacunacion']],\n",
    "                                                     columns=chunk['vacunado'])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "    \n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1)  # Merge all dfs\n",
    "    epi_vac = pd.DataFrame(merged_epivac.sum(numeric_only=True, axis=1))\n",
    "    epi_vac.columns = ['vacunados']\n",
    "    epi_vac['vacunados'] = epi_vac['vacunados'].astype(np.int64)\n",
    "    epi_vac.reset_index(level=0, inplace=True)\n",
    "\n",
    "    return epi_vac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vac_department(vac_url):\n",
    "    \"\"\"\n",
    "    Función que toma la dirección del dataset de vacunados y devuelve el número de VACUNADOS \n",
    "    por los 24 departamentos del Perú\n",
    "    \"\"\"\n",
    "    vac_col = ['id_centro_vacunacion', 'dosis','fecha_vacunacion']                     \n",
    "    df_vac = fn.read_largeCSV_file(vac_url, ',', vac_col)    \n",
    "    lst_vac = fn.df_into_chunks(df_vac)               \n",
    "    \n",
    "    for df in lst_vac:                                           \n",
    "        df = df.drop(df[df[\"dosis\"] == 1].index,  inplace=True)     # Drop non fully vaccinated (1 dose)\n",
    "\n",
    "    for df in lst_vac:\n",
    "        df['vacunado'] = 1  # To count each case\n",
    "        df['vacunado'] = df['vacunado'].apply(np.int8)\n",
    "        del df['dosis']     # Dose var is no needed anymore\n",
    "\n",
    "    return lst_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vac_url = \"RawData/TB_VACUNACION_COVID19.csv\"\n",
    "vacxdep = vac_department(vac_url)\n",
    "del vac_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(vacxdep):\n",
    "    chunk = fn.variable_fecha_ymd(chunk, \"fecha_vacunacion\")\n",
    "    print(vacxdep[i].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(vacxdep):\n",
    "    chunk = fn.date_to_epiweek(chunk, \"fecha_vacunacion\")\n",
    "\n",
    "print(vacxdep[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubigeo_url = 'RawData/TB_UBIGEOS.csv'\n",
    "vaccenter_url = 'RawData/TB_CENTRO_VACUNACION.csv'\n",
    "\n",
    "ubigeo = pd.read_csv(ubigeo_url, usecols = ['id_ubigeo', 'departamento'])\n",
    "vaccenter = pd.read_csv(vaccenter_url, usecols= ['id_centro_vacunacion','id_ubigeo'])\n",
    "\n",
    "del ubigeo_url, vaccenter_url\n",
    "\n",
    "vaccenter = vaccenter.merge(ubigeo, on = 'id_ubigeo', how = 'left')\n",
    "del vaccenter['id_ubigeo']\n",
    "\n",
    "print(\"Head of the merged dataframe (vaccenter) with: 'id_centro_vacunacion' and 'departamento'\") \n",
    "print(vaccenter.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = vacxdep[0].merge(vaccenter, on = 'id_centro_vacunacion', how = 'left')\n",
    "#del vacxdep_id['id_centro_vacunacion']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(vacxdep):\n",
    "    chunk = chunk.merge(vaccenter, on = 'id_centro_vacunacion', how = 'left')\n",
    "    vacxdep[i] = chunk\n",
    "\n",
    "print(vacxdep[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacxdep_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el TOTAL DE VACUNADOS por DEPARTAMENTO \n",
    "    del PERÚ (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index = [chunk['epi_year'], chunk['epi_week']],\n",
    "                                                     columns = [chunk['departamento']])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "\n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1)  # Merge all dfs\n",
    "    merged_epivac = merged_epivac.fillna(0).astype(np.int64)\n",
    "    merged_epivac = merged_epivac.groupby(level=0, axis=1).sum()\n",
    "    #epi_vac = pd.DataFrame(merged_epivac.sum(numeric_only=True, axis=1))\n",
    "    #epi_vac.columns = ['vacunados']\n",
    "    #epi_vac['vacunados'] = epi_vac['vacunados'].astype(np.int64)\n",
    "    #epi_vac.reset_index(level=0, inplace=True)\n",
    "\n",
    "    return merged_epivac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacxdepxsemEpi = vacxdep_chunks(vacxdep)\n",
    "print(vacxdepxsemEpi.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacxdepxsemEpi.to_csv('Data/vacunados_x_departamentos_x_semanaEpi.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
