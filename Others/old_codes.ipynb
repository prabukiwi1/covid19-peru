{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes cuando se trabajaba por chunks, o lista de dataframes. Ahora se utiliza la librería Dask en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el dataset de vacunados en una lista de chunks o dfs\n",
    "# (Para procesar información en bloques de 500 000) \n",
    "lst_vac = fn.df_into_chunks(df_vac)\n",
    "del df_vac\n",
    "\n",
    "[chunk.dropna(inplace=True) for chunk in lst_vac] # Drop na en caso de que existan\n",
    "\n",
    "# Que cada 'chunk' de lst_vac sea solamente cuando dosis = 2, es decir tengan 2 dosis\n",
    "lst_vac = [chunk.loc[chunk['dosis'] == '2'] for chunk in lst_vac]\n",
    "\n",
    "# Convertir a formato fecha la columna 'fecha vacunacion' y obtener su año y semana epi\n",
    "[fn.variable_fecha(chunk, 'fecha_vacunacion') for chunk in lst_vac]\n",
    "[fn.date_to_epiweek(chunk, 'fecha_vacunacion') for chunk in lst_vac]\n",
    "\n",
    "# Creamos columnas de 1 en cada chunk para contabilizar cada caso de vacunado\n",
    "for chunk in lst_vac:                                        \n",
    "    chunk['vacunado'] = 1\n",
    "    chunk['vacunado'].apply(np.int8)\n",
    "    del chunk['dosis']  # Borramos columna dosis llena de número 2\n",
    "del chunk # Borramos el último chunk que queda al final\n",
    "\n",
    "print(lst_vac[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epiweeks_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el total de VACUNADOS por semana y año \n",
    "    epidemiológico (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index=[chunk['epi_year'],\n",
    "                                                            chunk['epi_week']],\n",
    "                                                     columns=chunk['vacunado'])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "        \n",
    "    # Unimos todos los dfs sumados en uno solo\n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1) \n",
    "    epi_vac = pd.DataFrame(merged_epivac.sum(numeric_only=True, axis=1))\n",
    "    epi_vac.columns = ['vacunados']\n",
    "    \n",
    "    return epi_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vac_dose(vac_url):\n",
    "    \"\"\"\n",
    "    Función que toma la dirección del dataset de vacunados y devuelve el TOTAL DE DOSIS APLICADAS \n",
    "    (1ra,2nda y 3era dosis) por semana epidemiológica\n",
    "    \"\"\"\n",
    "    vac_cols = ['fecha_vacunacion', 'dosis']                     \n",
    "    df_vac = fn.read_largeCSV_file(vac_url, ',', vac_cols)    \n",
    "    lst_vac = fn.df_into_chunks(df_vac)               \n",
    " \n",
    "    for idx, chunk in enumerate(lst_vac):\n",
    "        chunk = fn.variable_fecha(chunk, 'fecha_vacunacion')\n",
    "\n",
    "    for idx, chunk in enumerate(lst_vac):\n",
    "        chunk = fn.date_to_epiweek(chunk, \"fecha_vacunacion\")\n",
    "        \n",
    "    return lst_vac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacDose_chunks(dfs_vac):\n",
    "    \"\"\"\n",
    "    Devuelve un dataframe con el TOTAL DE VACUNADOS por DEPARTAMENTO \n",
    "    del PERÚ (recibe una lista de dataframes o chunks)\n",
    "    \"\"\"\n",
    "    var_holder = {}     # Diccionario para guardar nombres                                             \n",
    "    lst_epi_vac = []    # Lista de dfs para cada sumatoria de chunks\n",
    "                                         \n",
    "    for i, chunk in enumerate(dfs_vac):\n",
    "        var_holder['epi_vac_' + str(i)]= pd.crosstab(index = [chunk['epi_year'], chunk['epi_week']],\n",
    "                                                     columns = [chunk['dosis']])\n",
    "        lst_epi_vac.append(var_holder['epi_vac_' + str(i)])\n",
    "\n",
    "    merged_epivac = pd.concat(lst_epi_vac, axis=1)  # Merge all dfs\n",
    "    merged_epivac = merged_epivac.fillna(0).astype(np.int64)\n",
    "    merged_epivac = merged_epivac.groupby(level=0, axis=1).sum()\n",
    "\n",
    "    return merged_epivac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes ya no usados para dataset de vacunados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Procesamos el dataset de vacunados\n",
    "\n",
    "### 3.1. Leemos el dataset\n",
    "\n",
    "#Debido al tamaño del dataset y para preservar recursos, no se procesará la información usando **Pandas**. En su lugar se utilizará el paquete **Dask**.\n",
    "\n",
    "vac_col = ['fecha_vacunacion','dosis']\n",
    "df_vac = dd.read_csv(vac_url, sep = \",\", usecols = vac_col, \n",
    "                     dtype = {'fecha_vacunacion':'int32', 'dosis': 'int8'})\n",
    "del vac_col\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos como vacunados solamente a personas con las 2 dosis\n",
    "df_vac = df_vac.loc[df_vac['dosis'] == 2]\n",
    "\n",
    "# Transformamos a formato fecha\n",
    "df_vac = df_vac.assign(fecha_vacunacion = dd.to_datetime(df_vac[\"fecha_vacunacion\"], format = \"%Y%m%d\", \n",
    "                                                         errors=\"coerce\"))\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el año y semana epidemiológica en una sola columna\n",
    "df_vac['epi_date'] = df_vac['fecha_vacunacion'].map(lambda date : Week.fromdate(date).isoformat())\n",
    "del df_vac['fecha_vacunacion'], df_vac['dosis']\n",
    "\n",
    "# Se separa la columna obtenida en 2, una para el año y otra para la semana epidemiológica\n",
    "df_vac[['year','epi_week']] = df_vac['epi_date'].str.split(\"W\", 1, expand=True)\n",
    "del df_vac['epi_date']\n",
    "df_vac['vacunados'] = 1\n",
    "df_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vac['year'] = df_vac['year'].astype('int16')\n",
    "df_vac['epi_week'] = df_vac['epi_week'].astype('int8')\n",
    "df_vac['vacunados'] = df_vac['vacunados'].astype('int8')\n",
    "df_vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosstab4dask(ddf):\n",
    "\n",
    "    lst = [] # Lista para almacenar la sumatoria de cada particion\n",
    "\n",
    "    for partition in range(0, ddf.npartitions):\n",
    "        lst.append(ddf.partitions[partition].compute().groupby(['year', 'epi_week']).count())\n",
    "\n",
    "    total_epivac = pd.concat(lst, axis=1).sum(numeric_only=True, axis=1).apply(np.int64)\n",
    "    del lst\n",
    "    total_epivac = pd.DataFrame(total_epivac)\n",
    "    total_epivac.columns = ['vacunados']\n",
    "    \n",
    "    return total_epivac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_vac = crosstab4dask(df_vac)\n",
    "del df_vac\n",
    "epi_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_epiweeks = pd.concat([epi_fal, epi_vac], axis=1)         \n",
    "merged_epiweeks = merged_epiweeks.fillna(value = 0) # Rellenamos vacíos o Nan values con 0\n",
    "del epi_fal, epi_vac\n",
    "\n",
    "# Cambiamos a int ya que existen Nan values y cambia a float64 automáticamente\n",
    "merged_epiweeks = merged_epiweeks.astype('int64')\n",
    "                                    \n",
    "merged_epiweeks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
